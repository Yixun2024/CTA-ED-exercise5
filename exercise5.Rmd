---
title: "exercise 5"
author: "yixun"
date: "`r Sys.Date()`"
output: html_document
---

```{r, message=F}
library(tidyverse)
library(stringr)
library(tidytext)
library(topicmodels)
library(gutenbergr)
library(scales)
library(tm)
library(ggthemes)
library(readr)
library(quanteda)
library(quanteda.textmodels)
```

```{r, message=F, eval=F}
devtools::install_github("matthewjdenny/preText")
library(preText)
```


#exercise
```{r}
#choose two books
mary <- gutenberg_download(c(134,3420))
```


```{r}
#create a new column "booknumber", if the id is 134, then the values is "feminist1", or "feminist2"
#do tokenizations, and ungrouping the data, and removing stop words.
mary_words <- mary %>%
  mutate(booknumber = ifelse(gutenberg_id==134, "feminist1","feminist2")) %>%
  unnest_tokens(word, text) %>%
  filter(!is.na(word)) %>%
  count(booknumber, word, sort = TRUE) %>%
  ungroup() %>%
  anti_join(stop_words)

#create a document term matrix
mary_dtm <- mary_words %>%
  cast_dtm(booknumber, word, n)

tm::inspect(mary_dtm)
```

#LDA(latent dirichlet allocation)
```{r}
# k means the number of topics to model
# control command includes additional control parameters, seed = 1234 makes sure repeatability of results
mary_lda <- LDA(mary_dtm, k = 10, control = list(seed = 1234))
```

```{r}
# tidy: extract topic information from the LDA model and organize it into a data frame
# matrix = "beta" means topic-term distribution matrix
mary_topics <- tidy(mary_lda, matrix = "beta")

head(mary_topics, n = 10)
```

```{r}
# group by mary_topics, use top_n() function to select the top ten most-weighted terms in each topic, arrange() means reorder by topics and weighted terms
mary_top_terms <- mary_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#visualize
mary_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 4) +
  scale_y_reordered() +
  theme_tufte(base_family = "Helvetica")
```

#plot the frequencies
```{r}
tidy_mary <- mary %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# count frequency
tidy_mary %>%
  count(word, sort=TRUE)

# according to feminist1 and feminist2 to calculate frequency
bookfreq <- tidy_mary %>%
  mutate(booknumber = ifelse(gutenberg_id==134, "feminist1", "feminist2")) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(booknumber, word) %>%
  group_by(booknumber) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(booknumber, proportion)

#visualize
ggplot(bookfreq, aes(x = feminist1, y = feminist2, color = abs(feminist1 - feminist2))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme_tufte(base_family = "Helvetica") +
  theme(legend.position="none", 
        strip.background = element_blank(), 
        strip.text.x = element_blank()) +
  labs(x = "Mary feminist 2", y = "Mary feminist 1") +
  coord_equal()
```

```{r}
mary <- mary %>%
  filter(!is.na(text))

mary_chapter <- mary %>%
  mutate(booknumber = ifelse(gutenberg_id==134, "feminist1", "feminist2")) %>%
  group_by(booknumber) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, booknumber, chapter)

mary_chapter_word <- mary_chapter %>%
  unnest_tokens(word, text)

mary_word_counts <- mary_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

mary_word_counts

mary_chapters_dtm <- mary_word_counts %>%
  cast_dtm(document, word, n)

tm::inspect(mary_chapters_dtm)
```


```{r}
mary_chapters_lda <- LDA(mary_chapters_dtm, k = 4, control = list(seed = 1234))
```

```{r}
# matrix = gamma extracts the topic distribution information for each document
mary_chapters_gamma <- tidy(mary_chapters_lda, matrix = "gamma")
mary_chapters_gamma
```

```{r}
# get title and chapter information for each document.
mary_chapters_gamma <- mary_chapters_gamma %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

mary_chapter_classifications <- mary_chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

mary_book_topics <- mary_chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)

# inner join book topics and book classifications by topic
mary_chapter_classifications %>%
  inner_join(mary_book_topics, by = "topic") %>%
  filter(title != consensus)

# augment() function is used to extrapolate the new document-term matrix using the already trained LDA model.
assignments <- augment(mary_chapters_lda, data = mary_chapters_dtm)
assignments

assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(mary_book_topics, by = c(".topic" = "topic"))

#visualize
assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  geom_text(aes(x = consensus, y = title, label = scales::percent(percent))) +
  theme_tufte(base_family = "Helvetica") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words assigned to",
       y = "Book words came from",
       fill = "% of assignments")
```
#validation
```{r}
#select sample from corp
corp <- corpus(mary, text_field = "text")
documents <- corp[sample(1:5000,1000)]
print(names(documents[1:10]))
```

```{r, eval = F}
# threshold  0.2 means words below this threshold will be considered infrequent and will be filtered out
preprocessed_documents <- factorial_preprocessing(
    documents,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.2,
    verbose = FALSE)
```

```{r, eval = F}
preText_results <- preText(
    preprocessed_documents,
    dataset_name = "Mary text",
    distance_method = "cosine",
    num_comparisons = 20,
    verbose = FALSE)

```


```{r,eval=FALSE}
preText_score_plot(preText_results)
```

